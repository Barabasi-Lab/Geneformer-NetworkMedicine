{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28b18e8e-92da-4c74-b4eb-d9c7f91dd345",
   "metadata": {},
   "source": [
    "# Package Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f3dab14-bfce-4fd2-8e77-a62968e38659",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/defrondeville.c/miniconda3/envs/LLM/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-02-24 15:31:34,454\tINFO util.py:159 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2024-02-24 15:31:35,048\tINFO util.py:159 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os \n",
    "import traceback\n",
    "import pickle as pk\n",
    "from pathlib import Path\n",
    "from datasets import Dataset\n",
    "import itertools\n",
    "from sklearn.utils import shuffle\n",
    "import seaborn as sns\n",
    "from geneformer import TranscriptomeTokenizer\n",
    "import copy\n",
    "import argparse\n",
    "import requests\n",
    "import polars as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# ML base imports\n",
    "from base_utils.ML_base import *\n",
    "\n",
    "# Properly sets up NCCV environment\n",
    "GPU_NUMBER = [i for i in range(torch.cuda.device_count())] \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join([str(s) for s in GPU_NUMBER])\n",
    "os.environ[\"NCCL_DEBUG\"] = \"INFO\"\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915a9526-e7e2-4e8d-b409-e9cbfadbf1a5",
   "metadata": {},
   "source": [
    "# Cell Data Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86a83023-04e1-40b7-b784-2800fd7d3dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom cell dataset class\n",
    "class CellData(Dataset):\n",
    "    def __init__(self, test = None, train = None, \n",
    "                 label = \"cell_type\", ID = 'ENSEMBL', dataset_normalization = True):\n",
    "        # Dataset normalization controls whether data is ranked based on just median normalized data, or if it is ranked based on median and dataset gene count normalization\n",
    "        self.dataset_normalization = dataset_normalization\n",
    "        try:\n",
    "            self.label = label\n",
    "            \n",
    "            # Uses a train/test split if provided, otherwise creates the test split if only a test value is provided\n",
    "            try:\n",
    "                train_test_labels = [1 for _ in range(len(train))] + [0 for _ in range(len(test))]\n",
    "                data = pl.concat((train, test), how = 'vertical')\n",
    "                data, ranked_genes = self.convert(data, label, ID, train_test_labels)\n",
    "            except:\n",
    "                data, ranked_genes = self.convert(test, label, ID, train_test_labels = None)\n",
    "                \n",
    "            self.ranked_genes = ranked_genes\n",
    "            super().__init__(data)\n",
    "        except:\n",
    "            print(traceback.format_exc())\n",
    "            pass\n",
    "        \n",
    "    def convert(self, data, label, ID, train_test_labels, count_id = 'expression', gene_id = \"genes\", GF_limit = 2048):\n",
    "        tokens = TranscriptomeTokenizer()\n",
    "        token_dict = tokens.gene_token_dict\n",
    "        median_dict = tokens.gene_median_dict\n",
    "        if train_test_labels != None:\n",
    "            ans_dict = {'input_ids':[], 'length':[], label:[], 'train':train_test_labels}\n",
    "        else:\n",
    "            ans_dict = {'input_ids':[], 'length':[], label:[]}\n",
    "        \n",
    "        # Normalizes for gene counts in sample\n",
    "        gene_counts = {}\n",
    "        for col_name in data.columns:\n",
    "            if '_' not in col_name:\n",
    "                gene_expression = data[col_name]\n",
    "                gene_counts[col_name] = np.median(gene_expression)\n",
    "                    \n",
    "        genes = list(gene_counts.keys())\n",
    "        expression = data.select(genes)\n",
    "        label_index = data.columns.index(label)\n",
    "    \n",
    "        \n",
    "        for row in expression.iter_rows():\n",
    "            gexp = {genes[i]:exp for i, exp in enumerate(row[:-1])}\n",
    "            \n",
    "            # Normalizes each set of genes by median and/or dataset gene count\n",
    "            for num, key in enumerate(list(gexp.keys())):\n",
    "                try:\n",
    "                    if self.dataset_normalization:\n",
    "                        gexp[key] /= (median_dict[key] * gene_counts[key])\n",
    "                    else:\n",
    "                        gexp[key] /= (median_dict[key])\n",
    "                except:\n",
    "                    gexp.pop(key)\n",
    "               \n",
    "                \n",
    "            gexp = sorted(gexp.items(), key = lambda x: x[1], reverse = True)\n",
    "            ranked_genes = [gexp[i][0] for i in range(len(gexp))][:GF_limit]\n",
    "            \n",
    "            input_ids = self.tokenize_dataset(gene_set = ranked_genes, token_dict = token_dict, type = ID)\n",
    "            ans_dict[\"input_ids\"].append(input_ids)\n",
    "            ans_dict[\"length\"].append(len(input_ids))\n",
    "            try:\n",
    "                ans_dict[label].append(row[label_index])\n",
    "            except:\n",
    "                ans_dict[label].append(row[label_index - 1])\n",
    "        \n",
    "        # Creates pyarrow tabe out of the data\n",
    "        data = pa.Table.from_arrays([ans_dict[key] for key in list(ans_dict.keys())], names=list(ans_dict.keys()))\n",
    "\n",
    "        return data, ranked_genes\n",
    "        \n",
    "    # Function for tokenizing genes into ranked-value encodings from Geneformer\n",
    "    def tokenize_dataset(self, gene_set, token_dict, type = None, species = 'human'):\n",
    "        wrap = True\n",
    "\n",
    "        if isinstance(gene_set[0], list) == False:\n",
    "            gene_set = [gene_set]\n",
    "            wrap = False\n",
    "            \n",
    "        pool = Pool()\n",
    "        converted_set = []\n",
    "\n",
    "        # Ensembl based searching\n",
    "        def process_gene(gene):\n",
    "             api_url = f\"https://rest.ensembl.org/xrefs/symbol/{species}/{gene}?object_type=gene\"\n",
    "             response = requests.get(api_url, headers={\"Content-Type\": \"application/json\"})\n",
    "             try:\n",
    "                 data = response.json()\n",
    "                 gene = data[0]['id']\n",
    "             except:\n",
    "                 gene = None\n",
    "             return gene\n",
    "             \n",
    "        # HGNC ID searching\n",
    "        def process_hgnc(gene):\n",
    "            for gene in tqdm.tqdm(genes, total = len(genes)):\n",
    "                api_url = f\"https://rest.ensembl.org/xrefs/symbol/{species}/{hgnc_id}?object_type=gene\"\n",
    "                response = requests.get(api_url, headers={\"Content-Type\": \"application/json\"})\n",
    "                try:\n",
    "                    data = response.json()\n",
    "                    gene = data[0]['id']\n",
    "                except:\n",
    "                    gene = None\n",
    "                return gene\n",
    "                        \n",
    "        # GO ID searching\n",
    "        def process_go(gene):\n",
    "             mg = mygene.MyGeneInfo()\n",
    "             results = mg.query(gene, scopes=\"go\", species=species, fields=\"ensembl.gene\")\n",
    "    \n",
    "             ensembl_ids = []\n",
    "             max_score = 0\n",
    "             for hit_num, hit in enumerate(results[\"hits\"]):\n",
    "                 if hit['_score'] > max_score:\n",
    "                     max_score = hit['_score']\n",
    "                     chosen_hit = hit\n",
    "             try:\n",
    "                 try:\n",
    "                     gene = chosen_hit[\"ensembl\"][\"gene\"]\n",
    "                 except:\n",
    "                     gene = chosen_hit[\"ensembl\"][0][\"gene\"]\n",
    "             except:\n",
    "                 gene = None\n",
    "             return gene\n",
    "             \n",
    "        # Selects the ID conversion to ensembl\n",
    "        if type == None or type.upper() == 'ENSEMBL':\n",
    "            converted_set = gene_set\n",
    "        elif type.upper() == 'GENE':\n",
    "            for genes in gene_set:\n",
    "                converted_genes = []\n",
    "                for result in tqdm.tqdm(pool.imap(process_gene, genes), total = len(genes)):\n",
    "                    converted_genes.append(result)\n",
    "                converted_set.append(converted_genes)\n",
    "                \n",
    "        elif type.upper() == 'GO':\n",
    "            for genes in gene_set:\n",
    "                converted_genes = []\n",
    "                for result in tqdm.tqdm(pool.imap(process_go, genes), total = len(genes)):\n",
    "                    converted_genes.append(result)\n",
    "                converted_set.append(converted_genes)\n",
    "                \n",
    "        elif type.upper() == 'HGNC':\n",
    "            for genes in gene_set:\n",
    "                converted_genes = []\n",
    "                for result in tqdm.tqdm(pool.imap(process_hgnc, genes), total = len(genes)):\n",
    "                    converted_genes.append(result)\n",
    "                converted_set.append(converted_genes)\n",
    "                \n",
    "        # Obtains Cheml ENSEMBL names for each gene if possible\n",
    "        Chembl = []\n",
    "        \n",
    "        for set_num, set in enumerate(converted_set):\n",
    "            Chembl.append([])\n",
    "            for gene in set:\n",
    "                if gene == None:\n",
    "                    Chembl[set_num].append(None)\n",
    "                else:\n",
    "                    try:\n",
    "                        Chembl[set_num].append(token_dict[gene])\n",
    "                    \n",
    "                    except:\n",
    "                        print(f'{gene} not found in tokenized dataset!')\n",
    "                        Chembl[set_num].append(None)\n",
    "    \n",
    "        if wrap == False:\n",
    "            Chembl = Chembl[0]\n",
    "        \n",
    "        return Chembl    \n",
    "\n",
    "# Function for filtering the sample of samples with much greater or fewer genes\n",
    "def filter_samples(data):\n",
    "    gene_columns = data.columns\n",
    "    \n",
    "    # Calculate the total count of genes for each sample by iterating through rows\n",
    "    total_gene_counts = [row.sum() for row in data.select(gene_columns).to_numpy()]\n",
    "   \n",
    "    # Create a Series from the list of sums and add it as a new column\n",
    "    data_with_total = data.with_columns(pl.Series(\"total_gene_count\", total_gene_counts))\n",
    "\n",
    "    # Calculate the mean and standard deviation of the total counts\n",
    "    mean_count = data_with_total[\"total_gene_count\"].mean()\n",
    "    std_dev = data_with_total[\"total_gene_count\"].std()\n",
    "\n",
    "    # Filter out samples with total count outside the specified range\n",
    "    lower_bound = mean_count - 3 * std_dev\n",
    "    upper_bound = mean_count + 3 * std_dev\n",
    "    filtered_data = data_with_total.filter((pl.col(\"total_gene_count\") >= lower_bound) &\n",
    "                                           (pl.col(\"total_gene_count\") <= upper_bound))\n",
    "\n",
    "    # Drop the 'total_gene_count' column if not needed\n",
    "    filtered_data = filtered_data.drop(\"total_gene_count\")\n",
    "\n",
    "    return filtered_data\n",
    "\n",
    "# Function for equalizing labels\n",
    "def equalize_data(data, label = 'RA'):\n",
    "    labels = data[label].to_list()\n",
    "    label_set = list(set(labels))\n",
    "    freq = {i:labels.count(i) for i in list(set(labels))}\n",
    "    classes = sorted(freq.items(), key = lambda x: x[1])\n",
    "    min_class, min_freq = classes[0][0], classes[0][1]\n",
    "    labels = [i[0] for i in classes]\n",
    "    labels = [i for i in labels if i != min_class]\n",
    "    data_columns = data.columns\n",
    "    \n",
    "    class_numbers = {key:0 for key in label_set}\n",
    "    data_rows = []\n",
    "    for row in data.iter_rows():  \n",
    "        row_label = row[-1]  \n",
    "    \n",
    "        if class_numbers[row_label] < min_freq:\n",
    "            class_numbers[row_label] += 1\n",
    "            data_rows.append(row)\n",
    "    data = pl.DataFrame(data_rows, schema = data_columns)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef810a11-d225-46de-a8c0-19844c39861d",
   "metadata": {},
   "source": [
    "# Primary Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81a318a6-fba1-41b3-a31f-3f490d199b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primary function for running Geneformer analysis\n",
    "def format_sci(data, \n",
    "               token_dictionary = Path('geneformer/token_dictionary.pkl'), \n",
    "               augment = False, \n",
    "               noise = None, \n",
    "               save = 'Genes.dataset', \n",
    "               gene_conversion = Path(\"geneformer/gene_name_id_dict.pkl\"), \n",
    "               target_label = \"RA\", \n",
    "               GF_samples = 20000, \n",
    "               equalize = True,\n",
    "               save_img = 'Scipher_Roc_data.png',\n",
    "               ensembl_convert = True,\n",
    "               epochs = 50,\n",
    "               filter_data = False, \n",
    "               normalize = True, \n",
    "               augment_combine = False,\n",
    "               keyword  = None):\n",
    "               \n",
    "    '''\n",
    "    KEY FUNCTION PARAMETERS\n",
    "    ------------------------------------------\n",
    "    data : csv\n",
    "        CSV file containing expression/labelled data to be loaded into the model\n",
    "\n",
    "    augment: bool, default = False\n",
    "        Chooses whether to create augmented data and use it as a training set (with the true dataset as the test set) or not.\n",
    "        \n",
    "    noise : None, float, default = None\n",
    "        If set to a float, noise equivalent to the noise * the original gene mean for each gene will be applied to the dataset.\n",
    "        \n",
    "    save : str, path, default = 'Genes.dataset'\n",
    "        Save name for the GF-compatible saved dataset created when converting to the proper dataset format.\n",
    "        \n",
    "    target_label : str, default = 'RA'\n",
    "        The name of the column in the csv dataset that contains class labels.\n",
    "        \n",
    "    GF_samples : int, default = 20000\n",
    "        The number of samples to augment in total. Each class is represented equally\n",
    "    \n",
    "    equalize : bool, default = True\n",
    "        Equalizes the dataset so that all classes are represented in equal amounts\n",
    "        \n",
    "    save_file : bool, None, default = Stats.png\n",
    "        Save file for the PR/ROC curve generated\n",
    "    \n",
    "    finetuned_model_location : str, default = 'Geneformer-finetuned'\n",
    "        Location where the finetuned model weights are saved\n",
    "\n",
    "    epochs : int\n",
    "        Number of epochs to train Geneformer\n",
    "        \n",
    "    filter_data : bool, default = False\n",
    "        Whether data should be filters for samples falling outside of a standard deviation of gene counts (by default, -3 to 3)\n",
    "        \n",
    "    augment_combine : bool, default = True\n",
    "        If data is augmented, the data is mxixed into the train/test set. If set to false, it will PURELY be used as training data, and the og dataset will be used for testing\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    cols = []\n",
    "    conversion = {}\n",
    "    data = pl.read_csv(data)\n",
    "        \n",
    "    token_dict = pk.load(open(token_dictionary, 'rb'))\n",
    "    gene_dict = pk.load(open(gene_conversion, 'rb'))\n",
    "    \n",
    "    if ensembl_convert == True:\n",
    "        # Converts from gene symbol to ensembl ID\n",
    "        for column in data.columns:\n",
    "            try:\n",
    "                ensembl = gene_dict[column.strip()]\n",
    "            except:\n",
    "                continue\n",
    "            try:\n",
    "                token_dict[ensembl]\n",
    "            except:\n",
    "                continue\n",
    "            cols.append(column)\n",
    "            conversion[column] = ensembl\n",
    "\n",
    "        keep = cols + [target_label]\n",
    "        data = data.select(keep)\n",
    "        data = data.rename(conversion)\n",
    "    \n",
    "    if equalize == True:\n",
    "        data = equalize_data(data)\n",
    "  \n",
    "    labels = [int(i) for i in list(data[target_label])]\n",
    "    data = data.sample(fraction = 1.0, shuffle = True)\n",
    "  \n",
    "    # Calculates dataset bias\n",
    "    dataset_bias = 1 - (labels.count(0)/labels.count(1))/2\n",
    "    \n",
    "    # Augments data\n",
    "    augmented_data = None\n",
    "    if augment == True:\n",
    "        #augmented_data = augment_data(data = data, selected_label = 'all', num_samples = GF_samples, polars = True, normalize = False)    \n",
    "        augmented_data = augment_data(data = data, selected_label = 0, num_samples = labels.count(1) - labels.count(0), polars= True, normalize = False)\n",
    "        augmented_data = augmented_data.sample(fraction = 1.0, shuffle = True)\n",
    "        \n",
    "    if augment_combine:\n",
    "        data = pd.concat((data.to_pandas(), augmented_data.to_pandas()), axis = 0)\n",
    "        data = pl.from_pandas(data)\n",
    "        augmented_data = None\n",
    "        \n",
    "    # Normalizes data if indicated\n",
    "    if normalize:\n",
    "        data = normalize_data(data, polars = True)\n",
    "        \n",
    "    if filter_data:\n",
    "        data = filter_samples(data)\n",
    "    \n",
    "    # Converts data to GF-applicable format\n",
    "    try:\n",
    "        cell_data = CellData(train = augmented_data, test = data, label = target_label)\n",
    "    except:\n",
    "        cell_data = CellData(train = None, test = data, label = target_label)\n",
    "\n",
    "    cell_data.save_to_disk(save)\n",
    "    '''\n",
    "    # If you want to load data instead of creating data for GeneFormer (for replicability), move the 2 lines of save code below to the prior section. \n",
    "    cell_data = CellData()\n",
    "    cell_data.load_from_disk(save)\n",
    "    '''\n",
    "\n",
    "    # Selects only genes that are exposed to GeneFormer\n",
    "    data = data.select(cell_data.ranked_genes + [target_label])\n",
    "    \n",
    "    try:\n",
    "        augmented_data = augmented_data.select(cell_data.ranked_genes + [target_label])\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Adds types of noise to data if indicated\n",
    "    if noise:\n",
    "        data = add_noise(data, noise = noise)     \n",
    "        \n",
    "    data = data.sample(fraction = 1.0, shuffle = True)\n",
    "    \n",
    "    # Differential expression analysis\n",
    "    fpr_de, tpr_de, auc_de = de_analysis(data, label_column = 'RA')\n",
    "    print(f'Differential Expression AUC: {auc_de}')\n",
    "    \n",
    "    # Calculates ROC curve for GeneFormer\n",
    "    fpr_gf, tpr_gf, auc_gf = finetune_cells(model_location = \"/work/ccnr/GeneFormer/GeneFormer_repo\", dataset = 'Scipher.dataset', \n",
    "                                            epochs = epochs, geneformer_batch_size = 100,\n",
    "            skip_training = False, label = \"RA\", inference = False, optimize_hyperparameters = False, device = device,\n",
    "            emb_extract = False, freeze_layers = 0, output_dir = 'GF-finetuned', max_lr = 1e-3)\n",
    "    \n",
    "    # Ensemble models\n",
    "    fpr_svc, tpr_svc, auc_svc = SVC_model(data,)\n",
    "    fpr_rf, tpr_rf, auc_rf = RandomForest(data)\n",
    "    fpr_xg, tpr_xg, auc_xg = XGBoost(data)\n",
    "    fpr_ffn, tpr_ffn, auc_ffn = FFN(test_data = data)\n",
    "        \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr_rf, tpr_rf, color='darkorange', lw=2, label=f'Random Forest (RF) AUC = {round(auc_rf, 2)}')\n",
    "    plt.plot(fpr_svc, tpr_svc, color='green', lw=2, label=f'Support Vector Machine (SVM) AUC = {round(auc_svc, 2)}')\n",
    "    plt.plot(fpr_ffn, tpr_ffn, color = 'blue', lw=2, label = f'Feedforward Neural Network (FFN) AUC = {round(auc_ffn, 2)}')\n",
    "    plt.plot(fpr_gf, tpr_gf, color = 'red', lw=2, label = f'GeneFormer (GF) AUC = {round(auc_gf, 2)}')\n",
    "    plt.plot(fpr_de, tpr_de, color = 'black', lw=2, label = f'Differential Expression (DE) AUC = {round(auc_de, 2)}')\n",
    "    plt.plot(fpr_xg, tpr_xg, color = 'orange', lw=2, label = 'XGBoost (XG) AUC = {round(auc_xg, 2)}')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Chance')\n",
    "    \n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve Comparison')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    \n",
    "    # Saves FPR/TPR data\n",
    "    img_data = {\n",
    "        'Geneformer FPR': fpr_gf,\n",
    "        'Geneformer TPR':tpr_gf,\n",
    "        'Feed Forward FPR':fpr_ffn,\n",
    "        'Feed Forward TPR':tpr_ffn,\n",
    "        'SVM FPR':fpr_svc,\n",
    "        'SVM TPR':tpr_svc,\n",
    "        'RF FPR':fpr_rf,\n",
    "        'RF TPR':tpr_rf\n",
    "        \n",
    "        }\n",
    "    img_data = pd.DataFrame.from_dict(img_data, orient='index')\n",
    "    img_data = img_data.transpose()\n",
    "    \n",
    "    if keyword == None:\n",
    "        img_data.to_csv('data_ROC.csv')\n",
    "        plt.show()\n",
    "    else:\n",
    "        img_data.to_csv(f'{keyword}_data_ROC.csv')\n",
    "        plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf5cb74-247d-4bc0-a1e6-bc4c8240ffbe",
   "metadata": {},
   "source": [
    "# Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e92e716d-aac3-4961-8584-9cc1071c7001",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29159/3859551608.py:52: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  gexp[key] /= (median_dict[key] * gene_counts[key])\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 897/897 [00:00<00:00, 18230.10 examples/s]\n",
      "/home/defrondeville.c/miniconda3/envs/LLM/lib/python3.9/site-packages/anndata/_core/anndata.py:183: ImplicitModificationWarning: Transforming to str index.\n",
      "  warnings.warn(\"Transforming to str index.\", ImplicitModificationWarning)\n",
      "Fitting size factors...\n",
      "... done in 0.15 seconds.\n",
      "\n",
      "Fitting dispersions...\n",
      "... done in 1.40 seconds.\n",
      "\n",
      "Fitting dispersion trend curve...\n",
      "... done in 0.15 seconds.\n",
      "\n",
      "Fitting MAP dispersions...\n",
      "... done in 1.55 seconds.\n",
      "\n",
      "Fitting LFCs...\n",
      "... done in 0.65 seconds.\n",
      "\n",
      "Refitting 0 outliers.\n",
      "\n",
      "Running Wald tests...\n",
      "... done in 0.61 seconds.\n",
      "\n",
      "/home/defrondeville.c/miniconda3/envs/LLM/lib/python3.9/site-packages/sklearn/base.py:458: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "/home/defrondeville.c/miniconda3/envs/LLM/lib/python3.9/site-packages/sklearn/base.py:458: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "/home/defrondeville.c/miniconda3/envs/LLM/lib/python3.9/site-packages/sklearn/base.py:458: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "/home/defrondeville.c/miniconda3/envs/LLM/lib/python3.9/site-packages/sklearn/base.py:458: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "/home/defrondeville.c/miniconda3/envs/LLM/lib/python3.9/site-packages/sklearn/base.py:458: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log2 fold change & Wald test p-value: condition 1 vs 0\n",
      "                 baseMean  log2FoldChange     lfcSE      stat    pvalue  \\\n",
      "ENSG00000244115  5.504495        0.018674  0.041086  0.454499  0.649470   \n",
      "ENSG00000204020  4.891277       -0.001145  0.043583 -0.026282  0.979032   \n",
      "ENSG00000197769  4.250484       -0.000325  0.046752 -0.006949  0.994456   \n",
      "ENSG00000141179  6.350873        0.012082  0.038248  0.315881  0.752093   \n",
      "ENSG00000105173  4.695315       -0.006137  0.044480 -0.137970  0.890264   \n",
      "...                   ...             ...       ...       ...       ...   \n",
      "ENSG00000213625  8.941217       -0.005180  0.032236 -0.160707  0.872324   \n",
      "ENSG00000138386  6.198562        0.002943  0.038714  0.076028  0.939397   \n",
      "ENSG00000161921  8.496854        0.006874  0.033069  0.207860  0.835338   \n",
      "ENSG00000184922  5.798247       -0.006533  0.040029 -0.163205  0.870357   \n",
      "ENSG00000170836  6.164849       -0.006460  0.038819 -0.166412  0.867832   \n",
      "\n",
      "                     padj  \n",
      "ENSG00000244115  0.999898  \n",
      "ENSG00000204020  0.999898  \n",
      "ENSG00000197769  0.999898  \n",
      "ENSG00000141179  0.999898  \n",
      "ENSG00000105173  0.999898  \n",
      "...                   ...  \n",
      "ENSG00000213625  0.999898  \n",
      "ENSG00000138386  0.999898  \n",
      "ENSG00000161921  0.999898  \n",
      "ENSG00000184922  0.999898  \n",
      "ENSG00000170836  0.999898  \n",
      "\n",
      "[1983 rows x 6 columns]\n",
      "RF AUC: 0.6062773593737794\n",
      "Differential Expression AUC: 0.6062773593737794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter:   0%|          | 0/897 [00:00<?, ? examples/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m run_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124marthritis\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124marthritis\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m run_type:\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mformat_sci\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTraining_Datasets/GSE97476_unique.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_img\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mRAROC.png\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mScipher.dataset\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m               \u001b[49m\u001b[43mequalize\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoise\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilter_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensembl_convert\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeyword\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43marthritis\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124marthritis2\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m run_type:\n\u001b[1;32m      8\u001b[0m     format_sci(data \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining_Datasets/GSE97810_unique.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m), save_img \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRAROC.png\u001b[39m\u001b[38;5;124m'\u001b[39m, save \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mScipher.dataset\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m      9\u001b[0m                equalize \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, augment \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m30\u001b[39m, noise \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m,\n\u001b[1;32m     10\u001b[0m                             normalize \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, ensembl_convert \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, keyword \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124marthritis2\u001b[39m\u001b[38;5;124m'\u001b[39m) \n",
      "Cell \u001b[0;32mIn[3], line 145\u001b[0m, in \u001b[0;36mformat_sci\u001b[0;34m(data, token_dictionary, augment, noise, save, gene_conversion, target_label, GF_samples, equalize, save_img, ensembl_convert, epochs, filter_data, normalize, augment_combine, keyword)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDifferential Expression AUC: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mauc_de\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    144\u001b[0m \u001b[38;5;66;03m# Calculates ROC curve for GeneFormer\u001b[39;00m\n\u001b[0;32m--> 145\u001b[0m fpr_gf, tpr_gf, auc_gf \u001b[38;5;241m=\u001b[39m \u001b[43mfinetune_cells\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_location\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/work/ccnr/GeneFormer/GeneFormer_repo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mScipher.dataset\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgeneformer_batch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m        \u001b[49m\u001b[43mskip_training\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRA\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minference\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimize_hyperparameters\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m        \u001b[49m\u001b[43memb_extract\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfreeze_layers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mGF-finetuned\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_lr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;66;03m# Ensemble models\u001b[39;00m\n\u001b[1;32m    151\u001b[0m fpr_svc, tpr_svc, auc_svc \u001b[38;5;241m=\u001b[39m SVC_model(data,)\n",
      "File \u001b[0;32m/work/ccnr/GeneFormer/GeneFormer_repo/Cell_classifier.py:337\u001b[0m, in \u001b[0;36mfinetune_cells\u001b[0;34m(token_set, median_set, pretrained_model, kfold, dataset, dataset_split, filter_cells, epochs, cpu_cores, geneformer_batch_size, optimizer, max_lr, num_gpus, max_input_size, lr_schedule_fn, warmup_steps, freeze_layers, emb_extract, max_cells, emb_layer, emb_filter, emb_dir, overwrite, label, data_filter, ROC_curve, device, forward_batch, model_location, skip_training, sample_data, inference, optimize_hyperparameters, output_dir)\u001b[0m\n\u001b[1;32m    333\u001b[0m tprs \u001b[38;5;241m=\u001b[39m []                                                                  \n\u001b[1;32m    334\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fold, (train_idx, val_idx) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(skf\u001b[38;5;241m.\u001b[39msplit(np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mlen\u001b[39m(true_labels)), true_labels)):\n\u001b[1;32m    335\u001b[0m      \n\u001b[1;32m    336\u001b[0m      \u001b[38;5;66;03m# reload pretrained model\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m      model \u001b[38;5;241m=\u001b[39m \u001b[43mBertForSequenceClassification\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m                                                   \u001b[49m\u001b[43mnum_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrained_labels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m                                                   \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m                                                   \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtrain() \n\u001b[1;32m    343\u001b[0m      \u001b[38;5;66;03m# Splitting the dataset\u001b[39;00m\n\u001b[1;32m    344\u001b[0m      train_fold \u001b[38;5;241m=\u001b[39m labeled_trainset\u001b[38;5;241m.\u001b[39mselect(train_idx)\n",
      "File \u001b[0;32m~/miniconda3/envs/LLM/lib/python3.9/site-packages/transformers/modeling_utils.py:2271\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2266\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[1;32m   2267\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2268\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2269\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2270\u001b[0m         )\n\u001b[0;32m-> 2271\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/LLM/lib/python3.9/site-packages/torch/nn/modules/module.py:927\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    923\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    924\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m    925\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m--> 927\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/LLM/lib/python3.9/site-packages/torch/nn/modules/module.py:579\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 579\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    581\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    582\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    583\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    584\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    589\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    590\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/LLM/lib/python3.9/site-packages/torch/nn/modules/module.py:579\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 579\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    581\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    582\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    583\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    584\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    589\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    590\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/LLM/lib/python3.9/site-packages/torch/nn/modules/module.py:579\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 579\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    581\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    582\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    583\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    584\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    589\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    590\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/LLM/lib/python3.9/site-packages/torch/nn/modules/module.py:602\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    598\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    599\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    600\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    601\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 602\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    603\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    604\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/miniconda3/envs/LLM/lib/python3.9/site-packages/torch/nn/modules/module.py:925\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    922\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m    923\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    924\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m--> 925\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/LLM/lib/python3.9/site-packages/torch/cuda/__init__.py:217\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    214\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    215\u001b[0m \u001b[38;5;66;03m# This function throws if there's a driver initialization error, no GPUs\u001b[39;00m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;66;03m# are found or any other error occurs\u001b[39;00m\n\u001b[0;32m--> 217\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;66;03m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;66;03m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;66;03m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    221\u001b[0m _tls\u001b[38;5;241m.\u001b[39mis_initializing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
     ]
    }
   ],
   "source": [
    "run_type = 'arthritis'\n",
    "if 'arthritis' in run_type:\n",
    "    format_sci(data = Path(\"Training_Datasets/GSE97476_unique.csv\"), save_img = 'RAROC.png', save = 'Scipher.dataset',\n",
    "               equalize = True, augment = False, epochs = 30, noise = 0.0, filter_data = True,\n",
    "                            normalize = False, ensembl_convert = False, keyword = 'arthritis') \n",
    "                            \n",
    "if 'arthritis2' in run_type:\n",
    "    format_sci(data = Path(\"Training_Datasets/GSE97810_unique.csv\"), save_img = 'RAROC.png', save = 'Scipher.dataset', \n",
    "               equalize = True, augment = False, epochs = 30, noise = 0.0,\n",
    "                            normalize = False, ensembl_convert = False, keyword = 'arthritis2') \n",
    "elif 'lung_cancer' in run_type:\n",
    "    format_sci(data = Path(\"Training_Datasets/lungCancer.csv\"), save_img = 'CancerROC.png', save = 'Scipher.dataset', equalize = True, filter_data = False, noise = 0, \n",
    "                                                                                    normalize = False, epochs = 8, ensembl_convert = False,  keyword = 'lung_cancer') \n",
    "    \n",
    "elif 'breast_cancer' in run_type:\n",
    "    format_sci(data = Path(\"Training_Datasets/breastCancer.csv\"), ensembl_convert = False, \n",
    "               save_img = 'CancerROC.png', save = 'Scipher.dataset', equalize = True, filter_data = False, noise = 0, keyword = 'breast_cancer', \n",
    "                                                                                    normalize = False, epochs = 20) \n",
    "elif 'carcinoma' in run_type:\n",
    "    format_sci(data = Path(\"Training_Datasets/Carcinoma.csv\"), save_img = 'CancerROC.png', \n",
    "                                                                save = 'Scipher.dataset', equalize = True, filter_data = False, noise = 0, keyword = 'carcinoma',\n",
    "                                                                                    normalize = True, epochs = 20) \n",
    "elif 'covid' in run_type:\n",
    "    format_sci(data = Path(\"Training_Datasets/COVID_half.csv\"), save_img = 'CV_ROC.png', keyword = 'covid',\n",
    "               save = 'Scipher.dataset', equalize = True, epochs = 5) \n",
    "elif 'carcinoma_single' in args.type:\n",
    "    format_sci(data = Path(\"Training_Datasets/carcinoma_sample.csv\"), save_img = 'CV_ROC.png', keyword = 'carcinoma_single',\n",
    "               save = 'Scipher.dataset', equalize = True, epochs = 10) \n",
    "elif 'amd' in run_type:\n",
    "    format_sci(data = Path(\"Training_Datasets/AMD_frac.csv\"), save_img = 'CV_ROC.png', keyword = 'macular degen',\n",
    "               save = 'Scipher.dataset', equalize = True, epochs = 60) \n",
    "else:\n",
    "    format_sci(data = Path(\"Training_Datasets/singleRA.csv\"), save_img = 'ArtitROC.png', save = 'Scipher.dataset', equalize = True, normalize = True, filter_data = True, keyword = 'RA-large',\n",
    "               epochs = 15) \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f53048-aa37-4eaa-8604-b70263dffef0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ff5fe8-e597-405e-84fe-24cdde922100",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
